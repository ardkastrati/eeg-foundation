{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9b1a80-cd86-4ed3-b427-f038f1e0601a",
   "metadata": {},
   "source": [
    "# Finetuning Notebook for NeuroBench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837bab3-41f2-49a8-b657-bd14a9871d66",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- size statistics on train/val/test jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2d7d-b21d-4383-ac28-f6f3616c10aa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec7e8af-477d-4d7a-b00d-4c6a7da090ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append(\"/home/maxihuber/eeg-foundation/\")\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9dfce-034a-4919-af8c-3641c0f272cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shorten Jsons for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d092671-f918-4fd8-9e8e-0f0d90627008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_json(data_index_path, store_path, num_samples):\n",
    "    with open(data_index_path, 'r') as f:\n",
    "        index_dict = json.load(f)\n",
    "    short_index_dict = {}\n",
    "    task_name = list(index_dict.keys())[0]\n",
    "    short_index_dict[task_name] = [sample for sample in index_dict[task_name][:num_samples]]\n",
    "    short_index_dict[\"task_type\"] = index_dict[\"task_type\"]\n",
    "    with open(store_path, 'w') as file:\n",
    "        json.dump(short_index_dict, file)\n",
    "        print(f\"Stored to {store_path}\")\n",
    "\n",
    "shorten_json(\"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_train.json\",\n",
    "             \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_train_small.json\",\n",
    "             50)\n",
    "\n",
    "shorten_json(\"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_val.json\",\n",
    "             \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_val_small.json\",\n",
    "             10)\n",
    "\n",
    "shorten_json(\"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_test.json\",\n",
    "             \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_test_small.json\",\n",
    "             20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105e272-9c4a-45a4-925e-d86516947e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Adjust pathnames from kard -> maxihuber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e8129-6507-4314-8c91-cc41d8c58093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_json_paths(data_index_path, store_path):\n",
    "    with open(data_index_path, 'r') as f:\n",
    "        index_dict = json.load(f)\n",
    "    \n",
    "    index_dict_new = {}\n",
    "    task_name = list(index_dict.keys())[0]\n",
    "    index_dict_new[task_name] = []  # Initialize the list for the task\n",
    "    \n",
    "    for sample in index_dict[task_name]:\n",
    "        new_input = []\n",
    "        for file_path in sample[\"input\"]:\n",
    "            # Remove the /itet-stor/kard path prefix and replace with /itet-stor/maxihuber\n",
    "            modified_path = file_path.replace(\"/itet-stor/kard\", \"/itet-stor/maxihuber\")\n",
    "            new_input.append(modified_path)\n",
    "        sample[\"input\"] = new_input\n",
    "        index_dict_new[task_name].append(sample)\n",
    "    \n",
    "    index_dict_new[\"task_type\"] = index_dict[\"task_type\"]\n",
    "    \n",
    "    with open(store_path, 'w') as file:\n",
    "        json.dump(index_dict_new, file)\n",
    "        print(f\"Stored to {store_path}\")\n",
    "\n",
    "adjust_json_paths(\"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_train.json\",\n",
    "                  \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_train.json\")\n",
    "\n",
    "adjust_json_paths(\"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_val.json\",\n",
    "                  \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_val.json\")\n",
    "\n",
    "adjust_json_paths(\"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_test.json\",\n",
    "                  \"/itet-stor/maxihuber/deepeye_storage/index_files/EEGEyeNet_Direction_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428f8de-eb21-492f-b8fd-a274c9a8d86a",
   "metadata": {},
   "source": [
    "### Load Train/Val/Test Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140a137e-6e06-483e-8ff6-f9722725fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_filepath = \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_prepared/\"\n",
    "\n",
    "class_name = \"Clinical\"\n",
    "time_col = \"Time in Seconds\"\n",
    "task_channels = set(['AF3', 'AF4', 'AF7', 'AF8', 'AFz', \n",
    "                     'C1', 'C2', 'C3', 'C4', 'C5', 'C6', \n",
    "                     'CP1', 'CP2', 'CP3', 'CP4', 'CP5', 'CP6', 'CPz', 'Cz', \n",
    "                     'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', \n",
    "                     'FC1', 'FC2', 'FC3', 'FC4', 'FC5', 'FC6', 'FCz', \n",
    "                     'FT7', 'FT8', 'Fp1', 'Fp2', 'Fz', 'Mastoids', \n",
    "                     'O1', 'O2', 'Oz', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', \n",
    "                     'PO3', 'PO4', 'PO7', 'PO8', 'POz', 'Pz', 'T7', 'T8', 'TP7', 'TP8', \n",
    "                     'Veog', 'X', 'Y', 'Z'])\n",
    "\n",
    "age = {\n",
    "    \"task_name\": \"Age\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/age.json\"\n",
    "}\n",
    "\n",
    "depression = {\n",
    "    \"task_name\": \"Depression\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_depression.json\"\n",
    "}\n",
    "\n",
    "parkinsons = {\n",
    "    \"task_name\": \"Parkinsons\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_parkinsons.json\"\n",
    "}\n",
    "\n",
    "schizophrenia = {\n",
    "    \"task_name\": \"Schizophrenia\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/cli_schizophrenia.json\"\n",
    "}\n",
    "\n",
    "sex = {\n",
    "    \"task_name\": \"Sex\",\n",
    "    \"task_type\": \"Classification\",\n",
    "    \"json_path\": \"/itet-stor/maxihuber/deepeye_storage/foundation_clinical_tasks/sex.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2ba553-0c71-4133-9175-500d9b25fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [06:15<00:00, 37.54s/it]\n",
      "Loading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:59<00:00, 47.98s/it]\n"
     ]
    }
   ],
   "source": [
    "json_path = age[\"json_path\"]\n",
    "task_type = age[\"task_type\"]\n",
    "task_name = age[\"task_name\"]\n",
    "\n",
    "def load_index(data_index_path):\n",
    "    with open(data_index_path, 'r') as f:\n",
    "        train_test_dict = json.load(f)\n",
    "    train_samples = train_test_dict['train']\n",
    "    test_samples = train_test_dict['test']\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def load_file_data(data_index, task_channels):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\"):\n",
    "        # Load and concatenate dataframe\n",
    "        input_files = sample[\"input\"]\n",
    "        df = pd.DataFrame()\n",
    "        for file in input_files:\n",
    "            file = prefix_filepath + file\n",
    "            with open(file, 'rb') as f:\n",
    "                df_new = pickle.load(f)\n",
    "                df = pd.concat([df, df_new], axis=0)\n",
    "        start, length = int(sample[\"start\"]), int(sample[\"length\"])\n",
    "        df = df.iloc[start:length, :]\n",
    "        # Add metadata\n",
    "        sr = int(1 / float(float(df[time_col].iloc[1]) - float(df[time_col].iloc[0])))\n",
    "        outputs[num_samples] = sample[\"output\"]\n",
    "        srs[num_samples] = sr\n",
    "        durs[num_samples] = len(df) / sr\n",
    "        channels[num_samples] = list(set(df.columns) & task_channels)\n",
    "        df = df[channels[num_samples]].astype(float)\n",
    "        signals = torch.tensor(df.to_numpy(), dtype=torch.float32).T\n",
    "        data[num_samples] = signals\n",
    "        num_samples += 1\n",
    "    return data, outputs, srs, durs, channels\n",
    "\n",
    "train_index, test_index = load_index(json_path)\n",
    "\n",
    "train_index = train_index[:10]\n",
    "test_index = test_index[:5]\n",
    "\n",
    "train_data, train_outputs, train_sr, train_dur, train_channels = load_file_data(train_index, task_channels)\n",
    "test_data, test_outputs, test_sr, test_dur, test_channels = load_file_data(test_index, task_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c4355-b503-4a2e-8470-85887d6901b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preload Data into Run Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ed819-ef4e-42e3-93c9-9749dfcd0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_to_channels(pretrain_index):\n",
    "    with open(pretrain_index, 'r') as file:\n",
    "        pretrain_index = json.load(file)\n",
    "    file_to_channels = {}\n",
    "    for ie in pretrain_index:\n",
    "        file_to_channels[ie[\"path\"]] = ie[\"good_channels\"] if \"good_channels\" in ie else ie[\"channels\"]\n",
    "    return file_to_channels\n",
    "\n",
    "def get_file_to_sr(pretrain_index):\n",
    "    with open(pretrain_index, 'r') as file:\n",
    "        pretrain_index = json.load(file)\n",
    "    file_to_sr = {}\n",
    "    for ie in pretrain_index:\n",
    "        file_to_sr[ie[\"path\"]] = ie[\"sr\"]\n",
    "    return file_to_sr\n",
    "\n",
    "\n",
    "\n",
    "def load_file_data(data_index, file_to_channels, file_to_sr):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\"):\n",
    "        input_files = sample[\"input\"]\n",
    "        df = pd.DataFrame()\n",
    "        df_channels = []\n",
    "        for file in input_files:\n",
    "            with open(file, 'rb') as f:\n",
    "                df_new = pickle.load(f)\n",
    "                df = pd.concat([df, df_new], axis=0)\n",
    "            #sr = file_to_sr[file]\n",
    "            sr = 500\n",
    "            #df_channels.append(file_to_channels[file])\n",
    "        df = df.loc[sample[\"start\"]:sample[\"start\"]+sample[\"length\"]-1, :]\n",
    "        assert df.shape[0] == sample[\"length\"], f\"df.shape[0]={df.shape[0]}, sample['length']={sample['length']}\"\n",
    "        outputs[num_samples] = sample[\"output\"]\n",
    "        srs[num_samples] = sr\n",
    "        durs[num_samples] = sample[\"length\"] / sr\n",
    "        #unique_channels = set(df_channels)\n",
    "        unique_channels = set([f\"E{i}\" for i in range(1,129)] + [\"AVG_REF\"])\n",
    "        channels[num_samples] = unique_channels\n",
    "        df = df.loc[:, df.columns.intersection(unique_channels)]\n",
    "        data[num_samples] = df\n",
    "        num_samples += 1\n",
    "    return data, outputs, srs, durs, channels\n",
    "\n",
    "file_to_channels = get_file_to_channels(pretrain_index)\n",
    "file_to_sr = get_file_to_sr(pretrain_index)\n",
    "\n",
    "train_data, train_outputs, train_sr, train_dur, train_channels = load_file_data(train_index, file_to_channels, file_to_sr)\n",
    "val_data, val_outputs, val_sr, val_dur, val_channels = load_file_data(val_index, file_to_channels, file_to_sr)\n",
    "test_data, test_outputs, test_sr, test_dur, test_channels = load_file_data(test_index, file_to_channels, file_to_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eee28f-4f61-45de-aae6-c10333cc8f0f",
   "metadata": {},
   "source": [
    "## EEG-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70195951-8bb9-4c86-b150-093cee423457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FT.__init__] Regression: out_dim=1\n",
      "[FT.__init__] Regression: out_dim=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.9 /itet-stor/maxihuber/net_scratch/conda_envs/faste ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /srv/beegfs01/projects/deepeye_storage/data/finetune_ckpts/Age exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                         | Type                                 | Params | Mode \n",
      "----------------------------------------------------------------------------------------------\n",
      "0 | encoder                      | EncoderViTRoPE                       | 23.1 M | train\n",
      "1 | finetune_time_transformer    | Flexible_RoPE_Layer_scale_init_Block | 1.8 M  | train\n",
      "2 | finetune_channel_transformer | SingleTransformerEncoderLayer        | 2.2 M  | train\n",
      "3 | head                         | Linear                               | 385    | train\n",
      "4 | criterion                    | MSELoss                              | 0      | train\n",
      "----------------------------------------------------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "23.1 M    Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.272   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 62. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation_step] y_hat=tensor([-0.8159], device='cuda:0'), y=tensor([70.], device='cuda:0') -> loss=5014.888671875\n",
      "[validation_step] y_hat=tensor([-0.8416], device='cuda:0'), y=tensor([79.], device='cuda:0') -> loss=6374.6845703125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2f7f7be45d472b94ad93bbed945dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training_step] y_hat=tensor([-0.7036], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([79.], device='cuda:0') -> loss=6352.6572265625\n",
      "[training_step] y_hat=tensor([-0.5966], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=4172.72412109375\n",
      "[training_step] y_hat=tensor([-0.2418], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=4127.005859375\n",
      "[training_step] y_hat=tensor([-0.1002], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([73.], device='cuda:0') -> loss=5343.64208984375\n",
      "[training_step] y_hat=tensor([0.2895], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([70.], device='cuda:0') -> loss=4859.55224609375\n",
      "[training_step] y_hat=tensor([0.4622], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([77.], device='cuda:0') -> loss=5858.0341796875\n",
      "[training_step] y_hat=tensor([0.5085], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([69.], device='cuda:0') -> loss=4691.091796875\n",
      "[training_step] y_hat=tensor([0.8921], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=3982.60205078125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation_step] y_hat=tensor([0.7757], device='cuda:0'), y=tensor([70.], device='cuda:0') -> loss=4792.00537109375\n",
      "[validation_step] y_hat=tensor([1.0515], device='cuda:0'), y=tensor([79.], device='cuda:0') -> loss=6075.96875\n",
      "[training_step] y_hat=tensor([1.1959], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([70.], device='cuda:0') -> loss=4734.001953125\n",
      "[training_step] y_hat=tensor([1.4763], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=3909.214599609375\n",
      "[training_step] y_hat=tensor([1.4598], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=3911.272705078125\n",
      "[training_step] y_hat=tensor([1.6852], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([73.], device='cuda:0') -> loss=5085.8046875\n",
      "[training_step] y_hat=tensor([2.0882], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([64.], device='cuda:0') -> loss=3833.0654296875\n",
      "[training_step] y_hat=tensor([2.0856], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([69.], device='cuda:0') -> loss=4477.53369140625\n",
      "[training_step] y_hat=tensor([2.4479], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([79.], device='cuda:0') -> loss=5860.21728515625\n",
      "[training_step] y_hat=tensor([2.6464], device='cuda:0', grad_fn=<ViewBackward0>), y=tensor([77.], device='cuda:0') -> loss=5528.45458984375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation_step] y_hat=tensor([2.3570], device='cuda:0'), y=tensor([70.], device='cuda:0') -> loss=4575.56884765625\n",
      "[validation_step] y_hat=tensor([2.9262], device='cuda:0'), y=tensor([79.], device='cuda:0') -> loss=5787.22509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ce5c7daca4446e95f44aacc8b70af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      4157.4736328125      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_rmse         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     64.47847747802734     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     4157.4736328125     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_rmse        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    64.47847747802734    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 4157.4736328125, 'test_rmse': 64.47847747802734}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################################################\n",
    "# Label Encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Collect all unique class labels from train and test outputs\n",
    "all_outputs = list(train_outputs.values()) + list(test_outputs.values())\n",
    "if isinstance(all_outputs[0], str):  # Only if outputs are strings (classification)\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_outputs)\n",
    "else:\n",
    "    label_encoder = None  # No encoding needed for regression\n",
    "\n",
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, data, outputs, srs, durs, channels, task_type, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "        self.srs = srs\n",
    "        self.durs = durs\n",
    "        self.channels = channels\n",
    "        self.task_type = task_type\n",
    "        self.label_encoder = label_encoder\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signals = self.data[idx]\n",
    "        output = self.outputs[idx]\n",
    "        sr = self.srs[idx]\n",
    "        dur = self.durs[idx]\n",
    "        channels = self.channels[idx]\n",
    "\n",
    "        if self.task_type == \"Classification\" and self.label_encoder is not None:\n",
    "            output = self.label_encoder.transform([output])[0]  # Encode the output label\n",
    "            output_tensor = torch.tensor(output, dtype=torch.long)\n",
    "        else:\n",
    "            output_tensor = torch.tensor([output], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"signals\": signals,\n",
    "            \"output\": output_tensor,\n",
    "            \"sr\": sr,\n",
    "            \"dur\": dur,\n",
    "            \"channels\": channels,\n",
    "        }\n",
    "        \n",
    "full_train_dataset = FinetuneDataset(train_data, train_outputs, train_sr, train_dur, train_channels, task_type=task_type, label_encoder=label_encoder)\n",
    "test_dataset = FinetuneDataset(test_data, test_outputs, test_sr, test_dur, test_channels, task_type=task_type, label_encoder=label_encoder)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Calculate lengths for train and validation sets\n",
    "total_size = len(full_train_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "#########################################################################################################\n",
    "# DataLoaders\n",
    "import torchaudio\n",
    "from src.data.transforms import (\n",
    "    crop_spg,\n",
    "    custom_fft,\n",
    "    normalize_spg,\n",
    ")\n",
    "\n",
    "self_win_shifts = [.25, .5, 1, 2, 4, 8]\n",
    "self_patch_size = 16\n",
    "self_win_shift_factor = .25\n",
    "self_max_win_shift = self_win_shifts[-1]\n",
    "self_max_y_datapoints = 4_000\n",
    "\n",
    "def get_nr_y_patches(win_size, sr):\n",
    "    return int((sr / 2 * win_size + 1) / self_patch_size)\n",
    "\n",
    "def get_nr_x_patches(win_size, dur):\n",
    "    win_shift = win_size * self_win_shift_factor\n",
    "    x_datapoints_per_second = 1 / win_shift\n",
    "    x_datapoints = dur * x_datapoints_per_second + 1\n",
    "    return int(x_datapoints // self_patch_size)\n",
    "\n",
    "channel_name_map_path = '/home/maxihuber/eeg-foundation/src/data/components/channels_to_id.json'\n",
    "with open(channel_name_map_path, \"r\") as file:\n",
    "    self_channel_name_map = json.load(file)\n",
    "\n",
    "def self_get_generic_channel_name(channel_name):\n",
    "    channel_name = channel_name.lower()\n",
    "    # Remove \"eeg \" prefix if present\n",
    "    if channel_name.startswith(\"eeg \"):\n",
    "        channel_name = channel_name[4:]\n",
    "    # Simplify names with a dash and check if it ends with \"-\"\n",
    "    if \"-\" in channel_name:\n",
    "        if channel_name.endswith(\"-\"):\n",
    "            return \"None\"\n",
    "        return channel_name.split(\"-\")[0]\n",
    "    return channel_name\n",
    "\n",
    "def self_encode_mean(mean, win_size):\n",
    "    y_datapoints = mean.shape[0]\n",
    "    encoded_mean = torch.zeros(self_max_y_datapoints)\n",
    "    step_size = int(self_max_win_shift // win_size)\n",
    "    end_idx = step_size * y_datapoints\n",
    "    indices = torch.arange(0, end_idx, step_size)\n",
    "    encoded_mean[indices] = mean.squeeze_().float()\n",
    "    encoded_mean.unsqueeze_(1)\n",
    "    return encoded_mean\n",
    "\n",
    "#########################################################################################################\n",
    "# collate_fn\n",
    "def sample_collate_fn(batch):\n",
    "\n",
    "    signals, output, sr, dur, channels = batch[0][\"signals\"], batch[0][\"output\"], batch[0][\"sr\"], batch[0][\"dur\"], batch[0][\"channels\"]\n",
    "\n",
    "    if dur > 3_600:\n",
    "        dur = 3_600\n",
    "        signals = signals[:, :3_600*sr]\n",
    "    \n",
    "    #print(f\"[collate_fn] sr={sr}\")\n",
    "    \n",
    "    # TODO: compute spectrograms for each win_size\n",
    "    # gives a new dimension (S) in batch\n",
    "    # need another extra transformer after the encoder\n",
    "    # (B, 1, H, W) -> (S*B, 1, H, W)\n",
    "    valid_win_shifts = [\n",
    "        win_shift\n",
    "        for win_shift in self_win_shifts\n",
    "        if get_nr_y_patches(win_shift, sr) >= 1\n",
    "        and get_nr_x_patches(win_shift, dur) >= 1\n",
    "    ]\n",
    "\n",
    "    # list holding assembled tensors for varying window shifts\n",
    "    full_batch = {}   \n",
    "\n",
    "    for win_size in valid_win_shifts:\n",
    "        \n",
    "        fft = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=int(sr * win_size),\n",
    "            win_length=int(sr * win_size),\n",
    "            hop_length=int(sr * win_size * self_win_shift_factor),\n",
    "            normalized=True,\n",
    "        )\n",
    "    \n",
    "        spg_list = []\n",
    "        chn_list = []\n",
    "        mean_list = []\n",
    "        std_list = []\n",
    "    \n",
    "        for signal, channel in zip(signals, channels):\n",
    "            \n",
    "            # Channel information\n",
    "            channel_name = self_get_generic_channel_name(channel)\n",
    "            channel = self_channel_name_map[channel_name] if channel_name in self_channel_name_map else self_channel_name_map[\"None\"]\n",
    "    \n",
    "            # Spectrogram Computation & Cropping\n",
    "            spg = fft(signal)\n",
    "            spg = spg**2\n",
    "            spg = crop_spg(spg, self_patch_size)\n",
    "            \n",
    "            H_new, W_new = spg.shape[0], spg.shape[1]\n",
    "            h_new, w_new = H_new // self_patch_size, W_new // self_patch_size\n",
    "    \n",
    "            # Prepare channel information (per-patch)\n",
    "            channel = torch.full((h_new, w_new), channel, dtype=torch.float16)\n",
    "            \n",
    "            spg, mean, std = normalize_spg(spg)\n",
    "            mean = self_encode_mean(mean, win_size)\n",
    "            std = self_encode_mean(std, win_size)\n",
    "            \n",
    "            spg_list.append(spg)\n",
    "            chn_list.append(channel)\n",
    "            mean_list.append(mean)\n",
    "            std_list.append(std)\n",
    "        \n",
    "        win_batch = torch.stack(spg_list)\n",
    "        win_channels = torch.stack(chn_list)\n",
    "        win_means = torch.stack(mean_list)\n",
    "        win_stds = torch.stack(std_list)\n",
    "        \n",
    "        win_batch.unsqueeze_(1)\n",
    "        win_channels = win_channels.flatten(1)\n",
    "        win_means = win_means.transpose(1, 2)\n",
    "        win_stds = win_stds.transpose(1, 2)\n",
    "        \n",
    "        full_batch[win_size] = {\n",
    "            \"batch\": win_batch,\n",
    "            \"channels\": win_channels,\n",
    "            \"means\": win_means,\n",
    "            \"stds\": win_stds\n",
    "        }\n",
    "        #print(f\"[collate_fn] win_size={win_size}: {win_batch.shape}\")\n",
    "        \n",
    "    # == Finished iterating over all possible window shifts\n",
    "   \n",
    "    return full_batch, output\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sample_collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "\n",
    "#########################################################################################################\n",
    "# Model\n",
    "# == Metrics ==\n",
    "def rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from src.models.mae_rope_encoder import EncoderViTRoPE\n",
    "from src.models.components.vit_rope import (\n",
    "    Flexible_RoPE_Layer_scale_init_Block,\n",
    "    FlexibleRoPEAttention,\n",
    "    compute_axial_cis,\n",
    "    select_freqs_cis,\n",
    ")\n",
    "from timm.models.vision_transformer import Mlp as Mlp\n",
    "\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "class SingleTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(SingleTransformerEncoderLayer, self).__init__()\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
    "\n",
    "    def forward(self, src):\n",
    "        return self.encoder_layer(src)\n",
    "\n",
    "def mean_aggregation(tokens):\n",
    "    return torch.mean(torch.stack(tokens), dim=0)\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchmetrics\n",
    "\n",
    "class FineTuningModel(L.LightningModule):\n",
    "    def __init__(self, encoder, frozen_encoder, task_type, learning_rate, mask_ratio):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        \n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # Pretrained network\n",
    "        self.encoder = encoder       \n",
    "        if frozen_encoder:\n",
    "            self.freeze_encoder()\n",
    "\n",
    "        # Finetuning network\n",
    "        self.finetune_time_transformer = Flexible_RoPE_Layer_scale_init_Block(\n",
    "            dim=384,\n",
    "            num_heads=6,\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            drop=0.0,\n",
    "            attn_drop=0.0,\n",
    "            drop_path=0.0,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            act_layer=nn.GELU,\n",
    "            Attention_block=FlexibleRoPEAttention,\n",
    "            Mlp_block=Mlp,\n",
    "            init_values=1e-4,\n",
    "        )\n",
    "        \n",
    "        # Single 1D transformer encoder layer\n",
    "        self.finetune_channel_transformer = SingleTransformerEncoderLayer(\n",
    "            d_model=384,  # Match the dimension used in finetune_time_transformer\n",
    "            nhead=6       # Number of heads in the multiheadattention models\n",
    "        )\n",
    "\n",
    "        # Modular aggregation method on channel tokens\n",
    "        self.win_shift_aggregation = mean_aggregation\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            out_dim = 1\n",
    "            print(f\"[FT.__init__] Regression: out_dim={out_dim}\")\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, out_dim)\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            out_dim = len(set(v for k, v in train_outputs.items()))\n",
    "            print(f\"[FT.__init__] Classification: out_dim={out_dim}\")\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, out_dim)\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize buffers for validation and test predictions and targets\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, full_x):\n",
    "        \n",
    "        x_embeds = {}\n",
    "        H_W = {}\n",
    "        \n",
    "        for win_size, x_win in full_x.items():\n",
    "            spgs = x_win[\"batch\"]\n",
    "            channels = x_win[\"channels\"]\n",
    "            means = x_win[\"means\"]\n",
    "            stds = x_win[\"stds\"]\n",
    "            B, C, H, W = spgs.shape\n",
    "            # TODO: split into less rows if necessary because of CUDA error\n",
    "            #nr_tokens = B * C * H * W\n",
    "            #if nr_tokens > max_nr_tokens:\n",
    "            #    pass\n",
    "            x_emb, _, _, nr_meta_patches = self.encoder(\n",
    "                x=spgs,\n",
    "                means=means,\n",
    "                stds=stds,\n",
    "                channels=channels,\n",
    "                win_size=win_size,\n",
    "                mask_ratio=self.mask_ratio,\n",
    "            )\n",
    "            # TODO: \n",
    "            x_embeds[win_size] = x_emb\n",
    "            H_W[win_size] = (H, W)\n",
    "            #print(f\"[FT.forward, after self.encoder] x_emb.shape: {x_emb.shape}\")\n",
    "\n",
    "        # Pass through time-transformer\n",
    "        for win_size, x_emb in x_embeds.items():\n",
    "            freqs_cis = select_freqs_cis(\n",
    "                self.encoder, self.encoder.encoder_freqs_cis, H_W[win_size][0], H_W[win_size][1], win_size, x_emb.device\n",
    "            )\n",
    "            x_emb = self.finetune_time_transformer(x_emb, freqs_cis=freqs_cis, nr_meta_tokens=nr_meta_patches)\n",
    "            #print(f\"[FT.forward, after self.time_transformer] x_emb.shape: {x_emb.shape}\")\n",
    "            x_emb = x_emb[:, 0]\n",
    "            #print(f\"[FT.forward, after time-token] x_emb.shape: {x_emb.shape}\")\n",
    "            x_embeds[win_size] = x_emb\n",
    "\n",
    "        # Pass through channel-transformer\n",
    "        tokens = []\n",
    "        for win_size, x_emb in x_embeds.items():\n",
    "            x_emb = x_emb.unsqueeze(0)\n",
    "            #print(f\"[FT.forward, before channel-token] x_emb.shape: {x_emb.shape}\")\n",
    "            x_emb = self.finetune_channel_transformer(x_emb)  # Adding a batch dimension\n",
    "            x_emb = x_emb[0, 0]\n",
    "            #print(f\"[FT.forward, after channel-token] x_emb.shape: {x_emb.shape}\")\n",
    "            tokens.append(x_emb)\n",
    "\n",
    "        #print(f\"[FT.forward] len(tokens): {len(tokens)}\")\n",
    "        # Average over all window shifts\n",
    "        smart_token = self.win_shift_aggregation(tokens)\n",
    "        #print(f\"[FT.forward] smart_token.shape: {smart_token.shape}\")\n",
    "\n",
    "        # Pass through head\n",
    "        y_hat = self.head(smart_token)\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y)\n",
    "        self.log('train_loss', loss)\n",
    "        print(f\"[training_step] y_hat={y_hat}, y={y} -> loss={loss}\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        print(f\"[validation_step] y_hat={y_hat}, y={y} -> loss={loss}\")\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = torch.argmax(y_hat, dim=1)\n",
    "            self.validation_step_outputs.append((y.cpu(), y_pred.cpu()))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.validation_step_outputs.append((y.cpu(), y_hat.cpu()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_true = torch.cat([x[0] for x in self.validation_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.validation_step_outputs], dim=0)\n",
    "            balanced_acc = balanced_accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "            self.log('val_balanced_accuracy', balanced_acc, prog_bar=True)\n",
    "        elif self.task_type == \"Regression\":\n",
    "            y_true = torch.cat([x[0] for x in self.validation_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.validation_step_outputs], dim=0)\n",
    "            rmse_value = rmse(y_true, y_pred)\n",
    "            self.log('val_rmse', rmse_value, prog_bar=True)\n",
    "\n",
    "        # Clear the buffer\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(input=y_hat, target=y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = torch.argmax(y_hat, dim=1)\n",
    "            self.test_step_outputs.append((y.cpu(), y_pred.cpu()))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.test_step_outputs.append((y.cpu(), y_hat.cpu()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_true = torch.cat([x[0] for x in self.test_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.test_step_outputs], dim=0)\n",
    "            balanced_acc = balanced_accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "            self.log('test_balanced_accuracy', balanced_acc, prog_bar=True)\n",
    "        elif self.task_type == \"Regression\":\n",
    "            y_true = torch.cat([x[0] for x in self.test_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.test_step_outputs], dim=0)\n",
    "            rmse_value = rmse(y_true, y_pred)\n",
    "            self.log('test_rmse', rmse_value, prog_bar=True)\n",
    "\n",
    "        # Clear the buffer\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.head.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "#########################################################################################################\n",
    "# Instantiate\n",
    "# Load the checkpoint\n",
    "chkpt_path = \"/itet-stor/maxihuber/net_scratch/checkpoints/977598/epoch=0-step=32807-val_loss=133.55.ckpt\"\n",
    "checkpoint = torch.load(chkpt_path, map_location=torch.device('cpu'))\n",
    "state_dict = checkpoint['state_dict']\n",
    "state_dict = {k.replace(\"net.encoder.\", \"\"): v for k, v in state_dict.items() if \"net.encoder.\" in k}\n",
    "\n",
    "# Initialize the encoder and load the state dict\n",
    "encoder = EncoderViTRoPE(channel_name_map_path)\n",
    "encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Instantiate the fine-tuning model\n",
    "fine_tuning_model = FineTuningModel(encoder=encoder, frozen_encoder=True, task_type=task_type, learning_rate=0.05, mask_ratio=0)\n",
    "\n",
    "#########################################################################################################\n",
    "# Load the checkpoint\n",
    "chkpt_path = \"/itet-stor/maxihuber/net_scratch/checkpoints/977598/epoch=0-step=32807-val_loss=133.55.ckpt\"\n",
    "checkpoint = torch.load(chkpt_path, map_location=torch.device('cpu'))\n",
    "state_dict = checkpoint['state_dict']\n",
    "state_dict = {k.replace(\"net.encoder.\", \"\"): v for k, v in state_dict.items() if \"net.encoder.\" in k}\n",
    "\n",
    "# Initialize the encoder and load the state dict\n",
    "encoder = EncoderViTRoPE(channel_name_map_path)\n",
    "encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Instantiate the fine-tuning model\n",
    "fine_tuning_model = FineTuningModel(encoder=encoder, frozen_encoder=True, task_type=task_type, learning_rate=0.001, mask_ratio=0)\n",
    "\n",
    "#########################################################################################################\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"/itet-stor/maxihuber/deepeye_storage/finetune_ckpts/{task_name}\",\n",
    "    filename=\"{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=2,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "trainer.fit(fine_tuning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.test(model=fine_tuning_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b791b7-42dc-4dbc-9ac9-9f4bd15bfe98",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94ab25-0207-4ad1-9f84-cd20a5a7b9f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Handling & Model Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c10a439-4077-46d6-97fb-0f8100db896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/maxihuber/eeg-foundation/src/models/components/Baselines')\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, outputs, task_type, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.outputs = outputs\n",
    "        self.task_type = task_type\n",
    "        self.label_encoder = label_encoder\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signals = self.data[idx]\n",
    "        output = self.outputs[idx]\n",
    "        \n",
    "        if self.task_type == \"Classification\" and self.label_encoder is not None:\n",
    "            output = self.label_encoder.transform([output])[0]  # Encode the output label\n",
    "            output_tensor = torch.tensor(output, dtype=torch.long)\n",
    "        else:\n",
    "            output_tensor = torch.tensor([output], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"signals\": signals,\n",
    "            \"output\": output_tensor,\n",
    "        }\n",
    "\n",
    "durs = [df.shape[1] for idx, df in train_data.items()] + [df.shape[1] for idx, df in test_data.items()]\n",
    "n_chns = [df.shape[0] for idx, df in train_data.items()] + [df.shape[0] for idx, df in test_data.items()]\n",
    "dur_90 = int(np.percentile(durs, 90))\n",
    "chn_90 = int(np.percentile(n_chns, 90))\n",
    "\n",
    "def pad_tensor(tensor, target_height, target_width):\n",
    "    current_height, current_width = tensor.shape\n",
    "\n",
    "    # Pad height if necessary\n",
    "    if current_height < target_height:\n",
    "        padding_height = target_height - current_height\n",
    "        padding = torch.zeros((padding_height, current_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    else:\n",
    "        tensor = tensor[:target_height, :]\n",
    "\n",
    "    # Pad width if necessary\n",
    "    if current_width < target_width:\n",
    "        padding_width = target_width - current_width\n",
    "        padding = torch.zeros((tensor.shape[0], padding_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=1)\n",
    "    else:\n",
    "        tensor = tensor[:, :target_width]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "train_data_pad = {k: pad_tensor(signals, chn_90, dur_90) for k, signals in train_data.items()}\n",
    "test_data_pad = {k: pad_tensor(signals, chn_90, dur_90) for k, signals in test_data.items()}\n",
    "\n",
    "full_train_dataset = SimpleDataset(train_data_pad, train_outputs, task_type=task_type, label_encoder=label_encoder)\n",
    "test_dataset = SimpleDataset(test_data_pad, test_outputs, task_type=task_type, label_encoder=label_encoder)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio, val_ratio = 0.8, 0.2\n",
    "\n",
    "# Calculate lengths for train and validation sets\n",
    "total_size = len(full_train_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "def sample_collate_fn(batch):\n",
    "    signals, output = batch[0][\"signals\"], batch[0][\"output\"]\n",
    "    signals = signals.unsqueeze(0)\n",
    "    print(f\"[sample_collate_fn] signals.shape: {signals.shape}\")\n",
    "    return signals, output\n",
    "\n",
    "# Define the baseline model class\n",
    "class BaselineModel(L.LightningModule):\n",
    "    def __init__(self, task_type, learning_rate):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if task_type == \"Regression\":\n",
    "            self.out_dim = 1\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.out_dim = len(set(v for k, v in train_outputs.items()))\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = torch.argmax(y_hat, dim=1)\n",
    "            self.validation_step_outputs.append((y.cpu(), y_pred.cpu()))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.validation_step_outputs.append((y.cpu(), y_hat.cpu()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_true = torch.cat([x[0] for x in self.validation_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.validation_step_outputs], dim=0)\n",
    "            balanced_acc = balanced_accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "            self.log('val_balanced_accuracy', balanced_acc, prog_bar=True)\n",
    "        elif self.task_type == \"Regression\":\n",
    "            y_true = torch.cat([x[0] for x in self.validation_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.validation_step_outputs], dim=0)\n",
    "            rmse_value = rmse(y_true, y_pred)\n",
    "            self.log('val_rmse', rmse_value, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_pred = torch.argmax(y_hat, dim=1)\n",
    "            self.test_step_outputs.append((y.cpu(), y_pred.cpu()))\n",
    "        elif self.task_type == \"Regression\":\n",
    "            self.test_step_outputs.append((y.cpu(), y_hat.cpu()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if self.task_type == \"Classification\":\n",
    "            y_true = torch.cat([x[0] for x in self.test_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.test_step_outputs], dim=0)\n",
    "            balanced_acc = balanced_accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "            self.log('test_balanced_accuracy', balanced_acc, prog_bar=True)\n",
    "        elif self.task_type == \"Regression\":\n",
    "            y_true = torch.cat([x[0] for x in self.test_step_outputs], dim=0)\n",
    "            y_pred = torch.cat([x[1] for x in self.test_step_outputs], dim=0)\n",
    "            rmse_value = rmse(y_true, y_pred)\n",
    "            self.log('test_rmse', rmse_value, prog_bar=True)\n",
    "\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6207e6f-10f8-4112-b9bd-169b0c870efd",
   "metadata": {},
   "source": [
    "# CNN ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f46ce-a91b-4cec-8589-c817ae4cc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.Baselines.DL_Models.torch_models.CNN.CNN import CNN\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sample_collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "\n",
    "class CNNBaselineModel(BaselineModel):\n",
    "    def __init__(self, task_type, learning_rate):\n",
    "        super(CNNBaselineModel, self).__init__(task_type, learning_rate)\n",
    "        \n",
    "        self.model = CNN(\n",
    "            model_name=None,\n",
    "            path=None,\n",
    "            loss=\"dice-loss\",\n",
    "            model_number=None,\n",
    "            batch_size=1,\n",
    "            input_shape=[chn_90, dur_90],\n",
    "            output_shape=self.out_dim\n",
    "        )\n",
    "\n",
    "baseline_model = CNNBaselineModel(task_type=task_type, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680de65-d9c4-4d75-9fbe-961f1ca3f707",
   "metadata": {},
   "source": [
    "# EEGNet ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a037d053-7035-40c4-944a-eaaa6bf1b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 16:27:29,093 - root - INFO - Dice weights: [0.86686687 0.10610611 0.02702703]\n",
      "2024-06-03 16:27:29,095 - root - INFO - Using loss fct: DiceLoss()\n",
      "2024-06-03 16:27:29,130 - root - INFO - Number of model parameters: 7196013\n",
      "2024-06-03 16:27:29,132 - root - INFO - Number of trainable parameters: 7196013\n"
     ]
    }
   ],
   "source": [
    "from src.models.components.Baselines.DL_Models.torch_models.EEGNet.eegNet import EEGNet\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sample_collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "\n",
    "class EEGNetBaselineModel(BaselineModel):\n",
    "    def __init__(self, task_type, learning_rate):\n",
    "        super(EEGNetBaselineModel, self).__init__(task_type, learning_rate)\n",
    "        \n",
    "        self.model = EEGNet(\n",
    "            model_name=None,\n",
    "            path=None,\n",
    "            loss=\"dice-loss\",\n",
    "            model_number=None,\n",
    "            batch_size=1,\n",
    "            input_shape=[chn_90, dur_90],\n",
    "            output_shape=self.out_dim\n",
    "        )\n",
    "\n",
    "baseline_model = EEGNetBaselineModel(task_type=task_type, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84e630-3b8f-4e34-9bf8-2bd4855f416e",
   "metadata": {},
   "source": [
    "# UNet ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a8138e8-339a-47ce-9c8f-93ab2bf7410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 14:47:14,547 - root - INFO - Dice weights: [0.86686687 0.10610611 0.02702703]\n",
      "2024-06-03 14:47:14,553 - root - INFO - Using loss fct: DiceLoss()\n"
     ]
    }
   ],
   "source": [
    "from src.models.components.Baselines.DL_Models.torch_models.UNet.UNet import UNet\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sample_collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sample_collate_fn)\n",
    "\n",
    "class UNetBaselineModel(BaselineModel):\n",
    "    def __init__(self, task_type, learning_rate):\n",
    "        super(UNetBaselineModel, self).__init__(task_type, learning_rate)\n",
    "        \n",
    "        self.model = UNet(\n",
    "            model_name=None,\n",
    "            path=None,\n",
    "            loss=\"dice-loss\",\n",
    "            model_number=None,\n",
    "            batch_size=1,\n",
    "            input_shape=[chn_90, dur_90],\n",
    "            output_shape=self.out_dim\n",
    "        )\n",
    "\n",
    "baseline_model = UNetBaselineModel(task_type=task_type, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecf807-a14d-4df4-95be-b66e02207289",
   "metadata": {},
   "source": [
    "# xDAWN + LDA ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0abee4-b00e-45a1-873d-f890d9f8ece0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25e4663a-3f56-46fa-b037-707a022d9805",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa41d18-c3c4-45c1-a392-cc026c818004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 6 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=6)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | criterion | MSELoss | 0      | train\n",
      "1 | model     | EEGNet  | 7.2 M  | train\n",
      "----------------------------------------------\n",
      "7.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 M     Total params\n",
      "28.784    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddf10f34915403498c24678f799e79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sample_collate_fn] signals.shape: torch.Size([1, 62, 112151])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x62 and 500x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     12\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     13\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback],\n\u001b[1;32m     14\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mbaseline_model, dataloaders\u001b[38;5;241m=\u001b[39mtest_loader)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 117\u001b[0m, in \u001b[0;36mBaselineModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    116\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 117\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_hat, y)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 106\u001b[0m, in \u001b[0;36mBaselineModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-foundation/src/models/components/Baselines/DL_Models/torch_models/EEGNet/eegNet.py:137\u001b[0m, in \u001b[0;36mEEGNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n\u001b[0;32m--> 137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m permute \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m permute\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/itet-stor/maxihuber/net_scratch/conda_envs/fastenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x62 and 500x1)"
     ]
    }
   ],
   "source": [
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"/itet-stor/maxihuber/deepeye_storage/finetune_ckpts/{task_name}_baseline\",\n",
    "    filename=\"{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "trainer.fit(baseline_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "trainer.test(model=baseline_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b280e9f-1b88-4e57-840f-77950ecd6b79",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Classification: Balanced Accuracy\n",
    "\n",
    "Regression: mean euclidean distance for 2d eyenet, else RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
