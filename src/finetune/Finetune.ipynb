{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9402f8d-92de-4fb6-869d-a824e810e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load data function\n",
    "def load_data(json_paths):\n",
    "    data = []\n",
    "    for path in json_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            data.extend(json.load(f)[\"task_name\"])\n",
    "    return data\n",
    "\n",
    "# Dataset class\n",
    "class EEGEyeNetDataset(Dataset):\n",
    "    def __init__(self, data, task_type):\n",
    "        self.data = data\n",
    "        self.task_type = task_type\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        input_files = sample[\"input\"]\n",
    "        start = sample[\"start\"]\n",
    "        length = sample[\"length\"]\n",
    "        \n",
    "        signals = []\n",
    "        for file in input_files:\n",
    "            with open(file, 'rb') as f:\n",
    "                signal = pickle.load(f)\n",
    "                signal = signal[:, start:start+length]\n",
    "                signals.append(signal)\n",
    "        \n",
    "        signals = np.concatenate(signals, axis=1)\n",
    "        signals = self.scaler.fit_transform(signals.T).T  # Standardize\n",
    "        \n",
    "        output = sample[\"output\"]\n",
    "        \n",
    "        return torch.tensor(signals, dtype=torch.float32), torch.tensor(output, dtype=torch.float32 if self.task_type == \"Regression\" else torch.long)\n",
    "\n",
    "# Load datasets\n",
    "train_json = \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_train.json\"\n",
    "val_json = \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_val.json\"\n",
    "test_json = \"/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_test.json\"\n",
    "\n",
    "train_data = load_data([train_json])\n",
    "val_data = load_data([val_json])\n",
    "test_data = load_data([test_json])\n",
    "\n",
    "task_type = \"Regression\"  # or \"Classification\" based on the task\n",
    "\n",
    "train_dataset = EEGEyeNetDataset(train_data, task_type)\n",
    "val_dataset = EEGEyeNetDataset(val_data, task_type)\n",
    "test_dataset = EEGEyeNetDataset(test_data, task_type)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# LightningModule class for fine-tuning\n",
    "class FineTuningModel(L.LightningModule):\n",
    "    def __init__(self, encoder, task_type, learning_rate=1e-4):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if task_type == \"Regression\":\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, 1)\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.head = nn.Linear(encoder.encoder_embed_dim, len(set([d['output'] for d in train_data])))  # Adjust output classes\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb, _, _, _ = self.encoder(x)\n",
    "        x_emb = x_emb[:, 0]  # Assuming the first token is the cls token\n",
    "        return self.head(x_emb)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.head.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Load the checkpoint\n",
    "chkpt_path = \"/itet-stor/maxihuber/net_scratch/checkpoints/977598/epoch=0-step=32807-val_loss=133.55.ckpt\"\n",
    "checkpoint = torch.load(chkpt_path, map_location=torch.device('cpu'))\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# Initialize the encoder and load the state dict\n",
    "encoder = ModularMaskedAutoencoderViTRoPE(channel_names_path='path_to_channel_names.json').encoder\n",
    "encoder.load_state_dict({k.replace(\"encoder.\", \"\"): v for k, v in state_dict.items() if \"encoder\" in k})\n",
    "\n",
    "# Instantiate the fine-tuning model\n",
    "fine_tuning_model = FineTuningModel(encoder, task_type)\n",
    "\n",
    "# Train the model\n",
    "trainer = L.Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.fit(fine_tuning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.validate(fine_tuning_model, val_dataloaders=val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
