{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ef8f6b-0107-441b-8a01-da0aa62fdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c70cc-a47b-4415-8f48-b7278608c47b",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b380273a-4aab-4653-85d5-cb07ac6fcb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train size: 12275\n",
      "Full test size: 2719\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'task_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 144\u001b[0m\n\u001b[1;32m    142\u001b[0m     test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets \u001b[38;5;241m=\u001b[39m load_file_data(test_index, task_channels)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m load_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets \u001b[38;5;241m=\u001b[39m load_file_data(train_index, \u001b[43mtask_channels\u001b[49m)\n\u001b[1;32m    145\u001b[0m     val_data, val_outputs, val_sr, val_dur, val_channels, val_datasets \u001b[38;5;241m=\u001b[39m load_file_data(val_index, task_channels)\n\u001b[1;32m    146\u001b[0m     test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets \u001b[38;5;241m=\u001b[39m load_file_data(test_index, task_channels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'task_channels' is not defined"
     ]
    }
   ],
   "source": [
    "eye_class = {\n",
    "    \"class_name\": \"EyeNet\",\n",
    "    \"time_col\": \"time\",\n",
    "    \"prefix_filepath\": \"/itet-stor/maxihuber/deepeye_storage/foundation_prepared/\",\n",
    "    \"load_mode\": 1\n",
    "}\n",
    "\n",
    "eye_dir_amp = {\n",
    "    \"task_name\": \"EyeNetDirectionAmp\",\n",
    "    \"task_type\": \"Regression\",\n",
    "    \"json_path\": ['/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_train.json',\n",
    "                 '/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_val.json',\n",
    "                 '/itet-stor/maxihuber/deepeye_storage/eegeyenet_tasks/EEGEyeNet_Direction_Amp_test.json'],\n",
    "    \"out_dim\": 1,\n",
    "}\n",
    "\n",
    "used_class = eye_class\n",
    "class_name = used_class[\"class_name\"]\n",
    "time_col = used_class[\"time_col\"]\n",
    "prefix_filepath = used_class[\"prefix_filepath\"]\n",
    "load_mode = used_class[\"load_mode\"]\n",
    "\n",
    "used_task = eye_dir_amp \n",
    "task_name = used_task[\"task_name\"]\n",
    "task_type = used_task[\"task_type\"]\n",
    "json_path = used_task[\"json_path\"]\n",
    "out_dim = used_task[\"out_dim\"]\n",
    "\n",
    "def load_index0(data_index_path):\n",
    "    with open(data_index_path, 'r') as f:\n",
    "        train_test_dict = json.load(f)\n",
    "    train_samples = train_test_dict['train']\n",
    "    test_samples = train_test_dict['test']\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def load_index1(data_index_paths):\n",
    "    all_samples = []\n",
    "    for data_index_path in data_index_paths:\n",
    "        with open(data_index_path, 'r') as f:\n",
    "            subset_dict = json.load(f)\n",
    "        all_samples.append(list(subset_dict.values())[0])\n",
    "    return all_samples[0], all_samples[1], all_samples[2]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"ERP_ERP_ANA\": 0, \"RS_RS_ALPHA\": 1, \"ERP_ERP_BISC\": 2, \"ERP_ERP_BBI\": 3, \n",
    "    \"ERP_ERP_BICF\": 4, \"ERP_ERP_BICD\": 5, \"RS_RS_SPIS\": 6, \"MI_MI_HGD\": 7, \n",
    "    \"MI_MI_SCP\": 8, \"ErrP_ErrP_MERP\": 9, \"MI_MI_ULM\": 10, \"MI_MI_VEP\": 11, \n",
    "    \"MI_MI_LR\": 12, \"MI_BBCI_IV_Graz_b\": 13, \"MI_MI_EB\": 14, \"MI_BBCI_IV_Graz_a\": 15, \n",
    "    \"MI_MI_GVH_V\": 16, \"MI_MI_GAL\": 17, \"MI_MI_Two\": 18, \"MI_MI_GVH_H\": 19, \n",
    "    \"MI_MI_II\": 20, \"ErrP_ErrP_BCI\": 21, \"MI_MI_GVH_G\": 22, \"MI_MI_Limb\": 23, \n",
    "    \"MI_MI_SCI\": 24, \"MI_BBCI_IV_Berlin\": 25, \"MI_eegmmidb\": 26, \"ERP_ERP_FHD\": 27, \n",
    "    \"RS_RS_EID\": 28\n",
    "}\n",
    "\n",
    "def extract_dataset_name(file_path, dataset_dict):\n",
    "    for name in dataset_dict.keys():\n",
    "        if name in file_path:\n",
    "            return name\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def load_file_data(data_index, task_channels):\n",
    "    num_samples = 0\n",
    "    data = {}\n",
    "    outputs = {}\n",
    "    srs = {}\n",
    "    durs = {}\n",
    "    channels = {}\n",
    "    datasets = {}\n",
    "    failed_samples = []\n",
    "\n",
    "    for sample in tqdm(data_index, desc=\"Loading data\", position=0, leave=True):\n",
    "        try:\n",
    "            # Load and concatenate dataframe\n",
    "            input_files = sample[\"input\"]\n",
    "            df = pd.DataFrame()\n",
    "            for file in input_files:\n",
    "                if load_mode != 1:\n",
    "                    file = prefix_filepath + file\n",
    "                else:\n",
    "                    file = file.replace(\"/itet-stor/kard\", \"/itet-stor/maxihuber\")\n",
    "                with open(file, 'rb') as f:\n",
    "                    df_new = pd.read_pickle(f)\n",
    "                    df = pd.concat([df, df_new], axis=0)\n",
    "                dataset_name = extract_dataset_name(file, dataset_dict)\n",
    "                datasets[num_samples] = dataset_name\n",
    "\n",
    "            start = int(sample[\"start\"])\n",
    "            length = int(sample[\"length\"]) if \"length\" in sample else int(sample[\"end\"])\n",
    "            if load_mode != 1:\n",
    "                df = df.iloc[start:length, :]\n",
    "            else:\n",
    "                df = df.loc[start:start+length, :]\n",
    "            \n",
    "            # Add metadata\n",
    "            if len(df) <= 1:\n",
    "                continue\n",
    "            sr = int(1 / float(float(df[time_col].iloc[1]) - float(df[time_col].iloc[0])))\n",
    "            if load_mode != 1:\n",
    "                outputs[num_samples] = sample[\"output\"] if \"output\" in sample else sample[\"label\"]\n",
    "            else:\n",
    "                if task_name == \"EyeNetPosition\":\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())\n",
    "                else:\n",
    "                    outputs[num_samples] = list(sample[\"output\"].values())[0]\n",
    "            srs[num_samples] = sr\n",
    "            durs[num_samples] = len(df) / sr\n",
    "            channels[num_samples] = list(set(df.columns) & task_channels)\n",
    "            df = df[channels[num_samples]].astype(float)\n",
    "            signals = torch.tensor(df.to_numpy(), dtype=torch.float32)\n",
    "            data[num_samples] = signals\n",
    "            num_samples += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sample: {sample}. Error: {e}\")\n",
    "            failed_samples.append(sample)\n",
    "    \n",
    "    return data, outputs, srs, durs, channels, datasets\n",
    "\n",
    "if load_mode == 0:\n",
    "    train_index, test_index = load_index0(json_path)\n",
    "elif load_mode == 1:\n",
    "    train_index, val_index, test_index = load_index1(json_path)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(f\"Full train size: {len(train_index)}\")\n",
    "print(f\"Full test size: {len(test_index)}\")\n",
    "\n",
    "if load_mode == 0:\n",
    "    train_index = train_index\n",
    "    test_index = test_index\n",
    "elif load_mode == 1:\n",
    "    train_index = train_index\n",
    "    val_index = val_index\n",
    "    test_index = test_index\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if load_mode == 0:\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = load_file_data(train_index, task_channels)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = load_file_data(test_index, task_channels)\n",
    "elif load_mode == 1:\n",
    "    train_data, train_outputs, train_sr, train_dur, train_channels, train_datasets = load_file_data(train_index, task_channels)\n",
    "    val_data, val_outputs, val_sr, val_dur, val_channels, val_datasets = load_file_data(val_index, task_channels)\n",
    "    test_data, test_outputs, test_sr, test_dur, test_channels, test_datasets = load_file_data(test_index, task_channels)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8eecf-eca8-46d7-b6e1-126af620dcd2",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72157685-8c95-48c6-b046-9f7c93863da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Label Encoder\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_outputs\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      5\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(train_outputs\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_outputs\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[1;32m      6\u001b[0m     label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if isinstance(list(train_outputs.values())[0], str):\n",
    "    all_outputs = list(set(list(train_outputs.values()) + list(test_outputs.values())))\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_outputs)\n",
    "\n",
    "    print(f\"Train classes: {set(train_outputs.values())}\")\n",
    "    print(f\"Test classes: {set(test_outputs.values())}\")\n",
    "else:\n",
    "    label_encoder = None\n",
    "\n",
    "# TODO (potentially): Class Weights\n",
    "\n",
    "# Ensure fixed input size for train/test set\n",
    "durs = [df.shape[1] for idx, df in train_data.items()] + [df.shape[1] for idx, df in test_data.items()]\n",
    "n_chns = [df.shape[0] for idx, df in train_data.items()] + [df.shape[0] for idx, df in test_data.items()]\n",
    "dur_90 = int(np.percentile(durs, 90))\n",
    "chn_90 = int(np.percentile(n_chns, 90))\n",
    "\n",
    "def pad_tensor(tensor, target_height, target_width):\n",
    "    current_height, current_width = tensor.shape\n",
    "    # Pad height if necessary\n",
    "    if current_height < target_height:\n",
    "        padding_height = target_height - current_height\n",
    "        padding = torch.zeros((padding_height, current_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    else:\n",
    "        tensor = tensor[:target_height, :]\n",
    "    # Pad width if necessary\n",
    "    if current_width < target_width:\n",
    "        padding_width = target_width - current_width\n",
    "        padding = torch.zeros((tensor.shape[0], padding_width), dtype=tensor.dtype)\n",
    "        tensor = torch.cat((tensor, padding), dim=1)\n",
    "    else:\n",
    "        tensor = tensor[:, :target_width]\n",
    "    # Return\n",
    "    return tensor\n",
    "\n",
    "train_data_pad = {k: pad_tensor(tensor=signals, target_width=chn_90, target_height=dur_90) for k, signals in train_data.items()}\n",
    "val_data_pad = {k: pad_tensor(tensor=signals, target_width=chn_90, target_height=dur_90) for k, signals in val_data.items()}\n",
    "test_data_pad = {k: pad_tensor(tensor=signals, target_width=chn_90, target_height=dur_90) for k, signals in test_data.items()}\n",
    "\n",
    "X_train = torch.stack(list(train_data_pad.values()), dim=0)\n",
    "y_train = torch.stack(list(train_outputs.values()), dim=0)\n",
    "\n",
    "X_val = torch.stack(list(val_data_pad.values()), dim=0)\n",
    "y_val = torch.stack(list(val_outpupts.values()), dim=0)\n",
    "\n",
    "X_test = torch.stack(list(test_data_pad.values()), dim=0)\n",
    "y_test = torch.stack(list(test_outputs.values()), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af891b-ad0e-45df-878b-11f497a66b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0ba7e-d392-4eec-9e03-c619060103ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(4*2*7, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.view(-1, 4*2*7)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
