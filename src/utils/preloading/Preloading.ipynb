{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd646f-f5b6-4e2c-bf04-a22cfde97f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6d4da7-5147-446b-9ddd-721121479622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 251])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from transforms import (\n",
    "    crop_spectrogram,\n",
    "    load_path_data,\n",
    "    load_channel_data,\n",
    "    fft_256,\n",
    "    custom_fft,\n",
    "    standardize,\n",
    ")\n",
    "\n",
    "# Example parameters\n",
    "n_fft = 256\n",
    "win_length = 256\n",
    "hop_length = 64\n",
    "\n",
    "# Create an example signal\n",
    "signal = torch.randn(16000)  # Example size, simulate 1 second at 16kHz\n",
    "\n",
    "# Initialize the Spectrogram transform\n",
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    normalized=True\n",
    ")\n",
    "crop = crop_spectrogram(target_size=[64,64])\n",
    "std = standardize()\n",
    "\n",
    "# Apply the transform\n",
    "spg = spectrogram(signal)\n",
    "print(spg.shape)\n",
    "spg = crop(spg)\n",
    "print(spg.shape)\n",
    "spg = std(spg)\n",
    "print(spg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed25a6c0-a67a-4185-8677-e1766f0593c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Dumped file!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_ie(index_element):\n",
    "    for k, v in index_element.items():\n",
    "        print(f\"{k}:\", v)\n",
    "\n",
    "index_path = '/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json'\n",
    "with open(index_path, 'r') as file:\n",
    "    index = json.load(file)\n",
    "\n",
    "short_index = index[:10]\n",
    "\n",
    "print(len(short_index))\n",
    "\n",
    "short_index_path = '/itet-stor/maxihuber/deepeye_storage/index_files/short_tueg_index.json'\n",
    "with open(short_index_path, 'w') as file:\n",
    "    json.dump(short_index, file, indent=4)\n",
    "    print(\"Dumped file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f8e021-c712-47e6-bfb7-79ad1b5ff5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_0.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_1.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_2.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_3.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_4.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_5.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_6.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_7.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_8.txt\n",
      "/itet-stor/maxihuber/net_scratch/runs/943432/tmp/index_path_arton06_9.txt\n",
      "2440011\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import json\n",
    "from socket import gethostname\n",
    "\n",
    "TMPDIR = '/itet-stor/maxihuber/net_scratch/runs/943432/tmp'\n",
    "\n",
    "file_paths = sorted(glob.glob(\n",
    "    os.path.join(TMPDIR, f\"index_path_{gethostname()}_*.txt\")\n",
    "))\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "\n",
    "paths = {}\n",
    "num_datapoints = 0\n",
    "\n",
    "for file_path in file_paths:\n",
    "\n",
    "    with open(file_path, \"r\") as pointer_file:\n",
    "        path_to_data_index = pointer_file.read()\n",
    "\n",
    "        with open(path_to_data_index, \"r\") as index_file:\n",
    "            chunks_index = json.load(index_file)\n",
    "\n",
    "            for _, chunk_path in chunks_index.items():\n",
    "\n",
    "                paths[num_datapoints] = chunk_path\n",
    "                num_datapoints += 1\n",
    "\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f932321c-2a75-4b6a-9d9c-89fc0f74401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing all files and subdirectories in: /dev/shm/mae\n",
      "Removed all files and subdirectories in /dev/shm/mae\n"
     ]
    }
   ],
   "source": [
    "# try teardown\n",
    "import shutil, os\n",
    "\n",
    "STORDIR = '/dev/shm/mae'\n",
    "if os.path.exists(STORDIR):\n",
    "    print(f\"Removing all files and subdirectories in: {STORDIR}\")\n",
    "    shutil.rmtree(STORDIR)\n",
    "    print(f\"Removed all files and subdirectories in {STORDIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f74fe9-aae3-43e6-bc4d-de4dd3a4418d",
   "metadata": {},
   "source": [
    "#Â Data Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143c75a5-1eff-4750-81ec-88f8b3080f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files. Currently at 0.02GB of data.\n",
      "Processed 10000 files. Currently at 243.42GB of data.\n",
      "Processed 20000 files. Currently at 477.77GB of data.\n",
      "Processed 30000 files. Currently at 742.3GB of data.\n",
      "Processed 40000 files. Currently at 967.9GB of data.\n",
      "Processed 50000 files. Currently at 1168.2GB of data.\n",
      "Processed 60000 files. Currently at 1386.76GB of data.\n",
      "Processed 0 files. Currently at 1638.91GB of data.\n",
      "Processed 10000 files. Currently at 1656.32GB of data.\n",
      "Processed 20000 files. Currently at 1674.38GB of data.\n",
      "Processed 30000 files. Currently at 1686.26GB of data.\n",
      "Processed 40000 files. Currently at 1696.85GB of data.\n",
      "Processed 50000 files. Currently at 1707.02GB of data.\n",
      "Processed 60000 files. Currently at 1717.68GB of data.\n",
      "Processed 70000 files. Currently at 1727.96GB of data.\n",
      "Processed 80000 files. Currently at 1738.5GB of data.\n",
      "Processed 90000 files. Currently at 1748.84GB of data.\n",
      "Processed 100000 files. Currently at 1759.4GB of data.\n",
      "Processed 110000 files. Currently at 1769.6GB of data.\n",
      "Processed 120000 files. Currently at 1779.6GB of data.\n",
      "Processed 130000 files. Currently at 1789.89GB of data.\n",
      "Processed 140000 files. Currently at 1799.85GB of data.\n",
      "Processed 150000 files. Currently at 1809.95GB of data.\n",
      "Processed 160000 files. Currently at 1820.42GB of data.\n",
      "Processed 170000 files. Currently at 1830.45GB of data.\n",
      "Processed 180000 files. Currently at 1840.78GB of data.\n",
      "Processed 190000 files. Currently at 1850.68GB of data.\n",
      "Processed 200000 files. Currently at 1861.34GB of data.\n",
      "Processed 210000 files. Currently at 1871.45GB of data.\n",
      "Processed 220000 files. Currently at 1881.55GB of data.\n",
      "Processed 230000 files. Currently at 1891.55GB of data.\n",
      "Processed 240000 files. Currently at 1901.94GB of data.\n",
      "Processed 250000 files. Currently at 1911.91GB of data.\n",
      "Processed 260000 files. Currently at 1922.05GB of data.\n",
      "Processed 270000 files. Currently at 1932.49GB of data.\n",
      "Processed 280000 files. Currently at 1942.74GB of data.\n",
      "Processed 290000 files. Currently at 1953.26GB of data.\n",
      "Processed 300000 files. Currently at 1963.75GB of data.\n",
      "Processed 310000 files. Currently at 1973.84GB of data.\n",
      "Processed 320000 files. Currently at 1984.26GB of data.\n",
      "Processed 330000 files. Currently at 1994.17GB of data.\n",
      "Processed 340000 files. Currently at 2004.07GB of data.\n",
      "417112\n",
      "2011.8099386477843\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "index_paths = [\"/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json\", \n",
    "               \"/itet-stor/maxihuber/deepeye_storage/index_files/full_pkl_index.json\"]\n",
    "path_prefix = '/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf'\n",
    "\n",
    "index = []\n",
    "\n",
    "total_size = 0\n",
    "\n",
    "with open(index_paths[0], 'r') as file:\n",
    "    tueg_index = json.load(file)\n",
    "\n",
    "print(len(tueg_index))\n",
    "\n",
    "\n",
    "tmp = \"\"\"\n",
    "for index_path in index_paths:\n",
    "    with open(index_path, 'r') as file:\n",
    "        temp_index = json.load(file)\n",
    "        for i, index_element in enumerate(temp_index):\n",
    "            if index_element[\"path\"].endswith(\".edf\"):\n",
    "                    index_element[\"path\"] = path_prefix + index_element[\"path\"]\n",
    "            index.append(index_element)\n",
    "            total_size += os.path.getsize(index_element[\"path\"]) / (1024 ** 3)\n",
    "            if i % 10_000 == 0:\n",
    "                print(f\"Processed {i} files. Currently at {round(total_size,2)}GB of data.\")\n",
    "\"\"\"\n",
    "print(len(index))\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf0143-ca54-4404-bb8d-c1afeb3d7c5e",
   "metadata": {},
   "source": [
    "# Get size of index contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efab933c-52df-4445-857d-fb0f6c90f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_sizes(index_paths: list):\n",
    "    index_sizes = [0] * len(index_paths)\n",
    "    for i, index_path in enumerate(index_paths):\n",
    "        with open(index_path, 'r') as file:\n",
    "            temp_index = json.load(file)\n",
    "            temp_index = temp_index[:len(temp_index) // 5]\n",
    "            for i, index_element in enumerate(temp_index):\n",
    "                if index_element[\"path\"].endswith(\".edf\"):\n",
    "                        index_element[\"path\"] = path_prefix + index_element[\"path\"]\n",
    "                total_size += os.path.getsize(index_element[\"path\"]) / (1024 ** 3)\n",
    "                if i % 10_000 == 0:\n",
    "                    print(f\"Processed {i} files. Currently at {round(total_size,2)}GB of data.\")\n",
    "            index_sizes[i] = total_size\n",
    "\n",
    "def get_index_size(index):\n",
    "    total_size = 0\n",
    "    for i, index_element in enumerate(index):\n",
    "        total_size += os.path.getsize(index_element[\"path\"]) / (1024 ** 3)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c31e84-3cca-426c-a88f-0af2db7d3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files. Currently at 0.02GB of data.\n",
      "Processed 10000 files. Currently at 243.42GB of data.\n",
      "13930\n",
      "311.0793815199286\n"
     ]
    }
   ],
   "source": [
    "index_paths = [\"/itet-stor/maxihuber/deepeye_storage/index_files/full_tueg_index.json\", \n",
    "               \"/itet-stor/maxihuber/deepeye_storage/index_files/full_pkl_index.json\"]\n",
    "path_prefix = '/itet-stor/maxihuber/deepeye_storage/foundation/tueg/edf'\n",
    "\n",
    "index = []\n",
    "total_size = 0\n",
    "\n",
    "for index_path in index_paths:\n",
    "    with open(index_path, 'r') as file:\n",
    "        temp_index = json.load(file)\n",
    "        temp_index = temp_index[:len(temp_index) // 5]\n",
    "        for i, index_element in enumerate(temp_index):\n",
    "            if index_element[\"path\"].endswith(\".edf\"):\n",
    "                    index_element[\"path\"] = path_prefix + index_element[\"path\"]\n",
    "            index.append(index_element)\n",
    "            total_size += os.path.getsize(index_element[\"path\"]) / (1024 ** 3)\n",
    "            if i % 10_000 == 0:\n",
    "                print(f\"Processed {i} files. Currently at {round(total_size,2)}GB of data.\")\n",
    "\n",
    "print(len(index))\n",
    "print(round(total_size,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2985fffd-25af-4ceb-9fab-15c2c5945901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13930\n",
      "Dumped!\n"
     ]
    }
   ],
   "source": [
    "with open(index_paths[0], 'r') as file:\n",
    "    temp_index = json.load(file)\n",
    "    temp_index = temp_index[:len(temp_index) // 5]\n",
    "    print(len(temp_index))\n",
    "\n",
    "assert False, \"break before dumping\"\n",
    "\n",
    "stor_path = '/itet-stor/maxihuber/deepeye_storage/index_files/311G_tueg_index.json'\n",
    "with open(stor_path, 'w') as file:\n",
    "    json.dump(temp_index, file)\n",
    "    print(\"Dumped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec8261-1b2b-4ec3-b8f0-35bd41339076",
   "metadata": {},
   "source": [
    "# Implement proportional-to-size splitting of index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09fba7c4-6db2-4138-b1ec-bb3052ade6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering data dir\n",
      "==========filtered 0 files==========\n",
      "==========filtered 10000 files==========\n",
      "==========filtered 20000 files==========\n",
      "==========filtered 30000 files==========\n",
      "==========filtered 40000 files==========\n",
      "==========filtered 50000 files==========\n",
      "==========filtered 60000 files==========\n",
      "==========filtered 70000 files==========\n",
      "==========filtered 80000 files==========\n",
      "==========filtered 90000 files==========\n",
      "==========filtered 100000 files==========\n",
      "==========filtered 110000 files==========\n",
      "==========filtered 120000 files==========\n",
      "==========filtered 130000 files==========\n",
      "==========filtered 140000 files==========\n",
      "==========filtered 150000 files==========\n",
      "==========filtered 160000 files==========\n",
      "==========filtered 170000 files==========\n",
      "==========filtered 180000 files==========\n",
      "==========filtered 190000 files==========\n",
      "==========filtered 200000 files==========\n",
      "==========filtered 210000 files==========\n",
      "==========filtered 220000 files==========\n",
      "==========filtered 230000 files==========\n",
      "==========filtered 240000 files==========\n",
      "==========filtered 250000 files==========\n",
      "==========filtered 260000 files==========\n",
      "==========filtered 270000 files==========\n",
      "==========filtered 280000 files==========\n",
      "==========filtered 290000 files==========\n",
      "==========filtered 300000 files==========\n",
      "==========filtered 310000 files==========\n",
      "==========filtered 320000 files==========\n",
      "==========filtered 330000 files==========\n",
      "==========filtered 340000 files==========\n",
      "==========filtered 0 files==========\n",
      "==========filtered 10000 files==========\n",
      "361390 files found in total\n",
      "[111858, 13213] files selected per index\n",
      "[327.54912449698895, 311.0599145554006] GB of data selected per index\n",
      "125071 files selected in total\n",
      "[55929, 55929, 4405, 4405, 4403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3]\n",
      "[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker 0 processes 55929 files (156.3 GB) from index 0 to 55929\n",
      "Worker 1 processes 55929 files (171.2 GB) from index 55929 to 111858\n",
      "Worker 2 processes 4405 files (127.4 GB) from index 111858 to 116263\n",
      "Worker 3 processes 4405 files (105.2 GB) from index 116263 to 120668\n",
      "Worker 4 processes 4403 files (78.4 GB) from index 120668 to 125071\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from math import ceil\n",
    "import sys\n",
    "# Add the directory containing your module to sys.path\n",
    "sys.path.append('/home/maxihuber/eeg-foundation/preloading')\n",
    "# Now you can import normally\n",
    "from preload_chunk import LocalLoader, filter_index, get_index_sizes\n",
    "\n",
    "def main(data_config, num_chunks, idx):\n",
    "    # num_chunks: number of workers available in the system\n",
    "    # idx: the id of the worker this program is called on (each worker executes this main method)\n",
    "\n",
    "    #TMPDIR = f\"{data_config['runs_dir']}/{os.environ['SLURM_ARRAY_JOB_ID']}/tmp\"\n",
    "    #os.makedirs(TMPDIR, exist_ok=True)\n",
    "\n",
    "    local_loader = LocalLoader(\n",
    "        base_stor_dir=data_config[\"STORDIR\"],\n",
    "    )\n",
    "\n",
    "    index, index_lens, index_sizes = filter_index(\n",
    "        index_paths=data_config[\"data_dir\"],\n",
    "        path_prefix=data_config[\"path_prefix\"],\n",
    "        min_duration=data_config[\"min_duration\"],\n",
    "        max_duration=data_config[\"max_duration\"],\n",
    "        select_sr=data_config[\"select_sr\"],\n",
    "        select_ref=data_config[\"select_ref\"],\n",
    "    )\n",
    "\n",
    "    # Calculate total size for normalization\n",
    "    total_size = sum(index_sizes)\n",
    "    \n",
    "    # Calculate number of chunks each root_directory should get based on size\n",
    "    # E.g. 6 workers for tueg data, 4 workers for pkl data (data is distributed to num_chunks workers in total)\n",
    "    num_chunks_per_directory = [ceil((size / total_size) * num_chunks) for size in index_sizes]\n",
    "\n",
    "    # Adjust chunk numbers to sum up to the total number of chunks available\n",
    "    while sum(num_chunks_per_directory) > num_chunks:\n",
    "        max_index = num_chunks_per_directory.index(max(num_chunks_per_directory))\n",
    "        num_chunks_per_directory[max_index] -= 1\n",
    "        \n",
    "    # Create global index chunks based on calculated per-directory chunks\n",
    "    start_idx = 0\n",
    "    global_index_chunks = []\n",
    "    for dir_idx, chunks in enumerate(num_chunks_per_directory):\n",
    "        # Compute the size of a chunk for this directory\n",
    "        # (e.g. 1'000 files per tueg worker, 20'000 files per pkl worker)\n",
    "        chunk_size = ceil(index_lens[dir_idx] / chunks)\n",
    "        for chunk_idx in range(chunks):\n",
    "            chunk_start = start_idx + chunk_idx * chunk_size\n",
    "            chunk_end = min(chunk_start + chunk_size, start_idx + index_lens[dir_idx])\n",
    "            global_index_chunks.append(index[chunk_start:chunk_end])\n",
    "        start_idx += index_lens[dir_idx]\n",
    "\n",
    "    # Assign chunk to current worker\n",
    "    len_index_chunks = [len(index_chunk) for index_chunk in global_index_chunks]\n",
    "    print(len_index_chunks, file=sys.stderr)\n",
    "\n",
    "    # Print information about global chunk distribution\n",
    "    processed = 0\n",
    "    for idx in range(num_chunks):\n",
    "        if idx < len(global_index_chunks):\n",
    "            index_chunk = global_index_chunks[idx]\n",
    "            print(\n",
    "                f\"Worker {idx} processes {len(index_chunk)} files ({round(get_index_size(index_chunk),1)} GB) from index {processed} to {processed + len(index_chunk)}\",\n",
    "                file=sys.stderr\n",
    "            )\n",
    "        else:\n",
    "            index_chunk = []\n",
    "            print(f\"Worker {idx} has no files to process.\", file=sys.stderr)\n",
    "        processed += len(index_chunk)\n",
    "\n",
    "    # Process the index_chunk for this idx...\n",
    "    index_chunk = global_index_chunks[idx] if idx < len(global_index_chunks) else []\n",
    "    \n",
    "    # (Add your processing logic here)\n",
    "\n",
    "num_chunks = 5\n",
    "idx = 0\n",
    "\n",
    "# Load main config file\n",
    "main_config_file = \"/home/maxihuber/eeg-foundation/configs/experiment/maxim.yaml\"\n",
    "with open(main_config_file, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    data_config = config[\"data\"]\n",
    "\n",
    "main(data_config, num_chunks, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0907c-af7a-4bb8-800a-948214adfa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
